use serde::{Deserialize, Serialize};
use tokio::time::{sleep, Duration};
use tracing::{debug, error, info};

/// Estrutura para uma mensagem do ChatGPT
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

/// Estrutura para a requisiÃ§Ã£o do ChatGPT
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatRequest {
    pub model: String,
    pub messages: Vec<ChatMessage>,
    pub temperature: f32,
    pub max_tokens: Option<u32>,
}

/// Estrutura para a resposta do ChatGPT
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatResponse {
    pub id: String,
    pub object: String,
    pub created: u64,
    pub model: String,
    pub choices: Vec<ChatChoice>,
    pub usage: Usage,
}

/// Estrutura para uma escolha da resposta
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatChoice {
    pub index: u32,
    pub message: ChatMessage,
    pub finish_reason: Option<String>,
}

/// Estrutura para o uso de tokens
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Usage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

/// Cliente para integraÃ§Ã£o com ChatGPT
pub struct ChatGPTClient {
    client: reqwest::Client,
    config: crate::config::Config,
    rate_limiter: tokio::sync::Mutex<()>,
}

impl ChatGPTClient {
    /// Cria um novo cliente ChatGPT
    pub fn new(config: crate::config::Config) -> crate::Result<Self> {
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(config.timeout_seconds))
            .build()
            .map_err(crate::Error::Http)?;

        Ok(Self {
            client,
            config,
            rate_limiter: tokio::sync::Mutex::new(()),
        })
    }

    /// Envia uma requisiÃ§Ã£o para o ChatGPT
    pub async fn send_request(&self, messages: Vec<ChatMessage>) -> crate::Result<ChatResponse> {
        let _rate_limit_guard = self.rate_limiter.lock().await;

        // Rate limiting
        sleep(Duration::from_millis(
            1000 / self.config.rate_limit_rps as u64,
        ))
        .await;

        let request = ChatRequest {
            model: self.config.model.clone(),
            messages,
            temperature: self.config.temperature,
            max_tokens: Some(4000), // Limite razoÃ¡vel para respostas
        };

        debug!("Enviando requisiÃ§Ã£o para ChatGPT: {:?}", request);

        let response = self
            .client
            .post(&self.config.openai_base_url)
            .header(
                "Authorization",
                format!("Bearer {}", self.config.openai_api_key),
            )
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await
            .map_err(|e| {
                error!("Erro na requisiÃ§Ã£o HTTP: {}", e);
                crate::Error::Http(e)
            })?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response
                .text()
                .await
                .unwrap_or_else(|_| "Erro desconhecido".to_string());

            error!("Erro na resposta do ChatGPT: {} - {}", status, error_text);

            return match status.as_u16() {
                429 => Err(crate::Error::RateLimit),
                401 => Err(crate::Error::Authentication(
                    "Chave API invÃ¡lida".to_string(),
                )),
                400 => Err(crate::Error::ChatGPT(format!(
                    "RequisiÃ§Ã£o invÃ¡lida: {}",
                    error_text
                ))),
                _ => Err(crate::Error::ChatGPT(format!(
                    "Erro HTTP {}: {}",
                    status, error_text
                ))),
            };
        }

        let chat_response: ChatResponse = response.json().await.map_err(|e| {
            error!("Erro ao deserializar resposta: {}", e);
            crate::Error::Http(e)
        })?;

        info!(
            "Resposta recebida do ChatGPT. Tokens usados: {}/{}",
            chat_response.usage.total_tokens, self.config.max_payload_tokens
        );

        Ok(chat_response)
    }

    /// Analisa um chunk de cÃ³digo com o ChatGPT e retorna relatÃ³rio Markdown
    pub async fn analyze_code_chunk(
        &self,
        chunk_content: &str,
        file_info: &str,
    ) -> crate::Result<String> {
        let system_message = ChatMessage {
            role: "system".to_string(),
            content: self.get_system_prompt().to_string(),
        };

        let user_message = ChatMessage {
            role: "user".to_string(),
            content: format!(
                "Analise o seguinte cÃ³digo e forneÃ§a um relatÃ³rio completo em Markdown:\n\n{}\n\n{}",
                file_info, chunk_content
            ),
        };

        let messages = vec![system_message, user_message];

        let response = self.send_request(messages).await?;

        if let Some(choice) = response.choices.first() {
            let content = &choice.message.content;
            return Ok(content.clone());
        }

        Err(crate::Error::ChatGPT(
            "Nenhuma resposta vÃ¡lida recebida".to_string(),
        ))
    }

    /// ObtÃ©m o prompt do sistema
    fn get_system_prompt(&self) -> &str {
        r#"VocÃª Ã© um especialista em seguranÃ§a de cÃ³digo e anÃ¡lise estÃ¡tica. 
Sua tarefa Ã© analisar cÃ³digo Python e identificar vulnerabilidades de seguranÃ§a, 
problemas de qualidade e oportunidades de melhoria.

IMPORTANTE: VocÃª deve retornar um relatÃ³rio completo formatado em MARKDOWN, nÃ£o JSON.

O relatÃ³rio deve incluir:

# RelatÃ³rio de AnÃ¡lise de SeguranÃ§a - CodeQL + ChatGPT

**Data:** [Data atual]  
**VersÃ£o:** 0.1.0  
**Gerado por:** Code Report

---

## ğŸ“Š Resumo Executivo

### EstatÃ­sticas Gerais
- **Total de achados:** [nÃºmero]
- **Arquivos com problemas:** [nÃºmero]
- **Score de risco mÃ©dio:** [0.0-1.0]

### DistribuiÃ§Ã£o por Severidade
- ğŸ”´ **Alta:** [nÃºmero] problemas
- ğŸŸ¡ **MÃ©dia:** [nÃºmero] problemas  
- ğŸŸ¢ **Baixa:** [nÃºmero] problemas

### Principais Descobertas
[Lista dos principais problemas encontrados]

---

## ğŸ“ˆ EstatÃ­sticas do CodeQL

- **Total de resultados:** [nÃºmero]
- **Arquivos com problemas:** [nÃºmero]

### DistribuiÃ§Ã£o por Severidade
- ğŸ”´ **Alta:** [nÃºmero] problemas
- ğŸŸ¡ **MÃ©dia:** [nÃºmero] problemas
- ğŸŸ¢ **Baixa:** [nÃºmero] problemas

---

## ğŸ” Achados Detalhados

### [Nome do Arquivo] - Linha [X]

**Problema:** [DescriÃ§Ã£o do problema]
**Severidade:** [Alta/MÃ©dia/Baixa]
**Categoria:** [SeguranÃ§a/Qualidade/Performance]
**Impacto:** [DescriÃ§Ã£o do impacto]
**RecomendaÃ§Ã£o:** [Como corrigir]

**CÃ³digo ProblemÃ¡tico:**
```python
[linha especÃ­fica do cÃ³digo com problema]
```

**CÃ³digo Corrigido:**
```python
[cÃ³digo corrigido com explicaÃ§Ã£o]
```

**Contexto do Problema:**
- **Arquivo:** [nome do arquivo]
- **Linha:** [nÃºmero da linha]
- **FunÃ§Ã£o:** [nome da funÃ§Ã£o se aplicÃ¡vel]
- **Severidade:** [Alta/MÃ©dia/Baixa]
- **CWE:** [CWE-ID se aplicÃ¡vel]

---

## ğŸ’¡ RecomendaÃ§Ãµes

### ğŸ”´ Prioridade Alta (Imediata)
[Lista de recomendaÃ§Ãµes crÃ­ticas]

### ğŸŸ¡ Prioridade MÃ©dia (PrÃ³ximas 2 semanas)
[Lista de recomendaÃ§Ãµes importantes]

### ğŸŸ¢ Prioridade Baixa (PrÃ³ximo mÃªs)
[Lista de melhorias gerais]

---

## ğŸ¯ Plano de AÃ§Ã£o

### ğŸ”´ Prioridade Alta (Imediata)
- [ ] [AÃ§Ã£o especÃ­fica]
- [ ] [AÃ§Ã£o especÃ­fica]

### ğŸŸ¡ Prioridade MÃ©dia (PrÃ³ximas 2 semanas)
- [ ] [AÃ§Ã£o especÃ­fica]
- [ ] [AÃ§Ã£o especÃ­fica]

### ğŸŸ¢ Prioridade Baixa (PrÃ³ximo mÃªs)
- [ ] [AÃ§Ã£o especÃ­fica]
- [ ] [AÃ§Ã£o especÃ­fica]

---

## ğŸ“‹ Metadados

**ConfiguraÃ§Ãµes utilizadas:**
- Modelo: gpt-3.5-turbo
- Temperatura: 0.2
- Rate limit: 30 req/s
- Timeout: 30s

---
*RelatÃ³rio gerado automaticamente pelo Code Report v0.1.0*

Seja objetivo, tÃ©cnico e acionÃ¡vel. Priorize seguranÃ§a e qualidade. Use emojis e formataÃ§Ã£o Markdown para melhor legibilidade."
  ],
  "risk_score": 0.75
}

Seja objetivo, tÃ©cnico e acionÃ¡vel. Priorize seguranÃ§a e qualidade."#
    }
}
